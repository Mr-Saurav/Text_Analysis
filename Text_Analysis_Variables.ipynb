{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b4e05f-26bf-4b79-b7bd-4905c6254a0a",
   "metadata": {},
   "source": [
    "# Variables for Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff989ec8-d473-4c64-aca4-c936a5c60d88",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6c4846-b837-45d3-a027-03d617c6a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from openpyxl import Workbook, load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de875f-14a4-4c28-a9c7-766a5e7c71cf",
   "metadata": {},
   "source": [
    "##### REQUIREMENTS: \n",
    "1. All Webpages must be article/blog webpage. And all should be the same company webpages.\n",
    "2. Need to upload the \"Input_file\"(excel file which contain the links of webpages of particular company) inside environment. \n",
    "3. Also Required to upload positive words & negative words txt file in “MastersDictionary” folder and stop words txt files in “StopWords” folder (remember to give the same name to the folder as it mentioned here, otherwise it creates discrepancies) inside the environment.\n",
    "4. Provide the number of wepages link that excel file contain in \"num_of_files\".\n",
    "5. Provide \"folder_name\" (make sure folder is create first at environment with same name). Here, we give \"extracted_articles\" as folder name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e81e9-6cef-4278-a651-88b46134d724",
   "metadata": {},
   "source": [
    "## Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e723af37-751a-4af5-94cc-1dbdb944cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the input.xlsx file inside python environment\n",
    "\n",
    "def store_articles(Input_file, num_of_files, folder_name):\n",
    "    \n",
    "    # make a dataframe where we load data present in input file\n",
    "    input_df = pd.read_excel(Input_file)\n",
    "\n",
    "    # through iteration we get url from dataframe we made\n",
    "    for i in range(num_of_files):\n",
    "        url = input_df.URL[i]\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # using BeautifulSoup for scraping the data\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        #change the class according to the webpage\n",
    "        title = soup.find('h1', attrs={'class': 'entry-title'})\n",
    "        if title is None:\n",
    "            title = soup.find('h1', attrs={'class':'tdb-title-text'})\n",
    "\n",
    "        content = soup.find('div', attrs={'class': 'td-post-content tagdiv-type'})\n",
    "        if content is None:\n",
    "            content = soup.find('div', attrs={'class': 'tdb-block-inner td-fix-index'})\n",
    "\n",
    "        # check if title and content are found\n",
    "        if title is not None and content is not None:\n",
    "            f_title = title.text\n",
    "            f_content = content.text.replace('\\n', '')\n",
    "\n",
    "            # storing each Article in folder \"extracted_articles\" with name \"webpage0001, webpage0002,....,etc\"\n",
    "            name = folder_name + '/webpage' + str(\"{:04d}\".format(i+1)) + '.txt'\n",
    "            with open(name, \"w\") as file:\n",
    "                file.write(f_title + \"\\n\\n\" + f_content)\n",
    "                file.flush()\n",
    "        else:\n",
    "            print(f'Webpage {str(\"{:04d}\".format(i+1))} have 404 Error')\n",
    "            \n",
    "    print(f\"\\n\\n All {num_of_files} webpages loaded as txt in {folder_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165871cb-2c92-4079-ba39-332480a30191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to create a list of stop words\n",
    "def make_stopword_list(stopword_file_list):\n",
    "    # store extracted data from Stop_words txt files into a list\n",
    "    ex_words = []\n",
    "    for i in stopword_file_list:\n",
    "        name = 'StopWords/' + i + '.txt'\n",
    "        with open(name, 'r', encoding='latin-1') as file:\n",
    "            for word in file.readlines():\n",
    "                lines = word.strip().split('|')\n",
    "                clean_word = [line.strip() for line in lines]\n",
    "                ex_words.extend(clean_word)\n",
    "    return ex_words\n",
    "\n",
    "#functions to create a list of positive words\n",
    "def make_pos_words_list(pos_file):\n",
    "    # store extracted data from positive_words txt files into a list\n",
    "    pos_words = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as pfile:\n",
    "        for word in pfile.readlines():\n",
    "            pos_words.append(word.replace('\\n',''))\n",
    "    return pos_words\n",
    "\n",
    "#functions to create a list of negative words\n",
    "def make_neg_words_list(neg_file):\n",
    "    # store extracted data from negative_words txt files into a list\n",
    "    neg_words = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as nfile:\n",
    "        for word in nfile.readlines():\n",
    "            neg_words.append(word.replace('\\n',''))\n",
    "    return neg_words\n",
    "\n",
    "#functions to create a dictionary of positive words\n",
    "def make_positive_dict(stopword_file_list, pos_file):\n",
    "    stop_words = make_stopword_list(stopword_file_list)\n",
    "    positive_words = make_pos_words_list(pos_file)\n",
    "    pos_dict = {}\n",
    "    i = 1\n",
    "    for word in positive_words:\n",
    "        if word not in stop_words:\n",
    "            pos_dict[i] = word\n",
    "            i = i+1\n",
    "    return pos_dict\n",
    "\n",
    "#functions to create a dictionary of negative words\n",
    "def make_negative_dict(stopword_file_list, neg_file):\n",
    "    stop_words = make_stopword_list(stopword_file_list)\n",
    "    negative_words = make_neg_words_list(neg_file)\n",
    "    neg_dict = {}\n",
    "    i = 1\n",
    "    for word in negative_words:\n",
    "        if word not in stop_words:\n",
    "            neg_dict[i] = word\n",
    "            i = i+1\n",
    "    return neg_dict\n",
    "\n",
    "#function to create a list of words(excluding stop words) in a webpage\n",
    "def content_without_stop_words(content):\n",
    "    #removing all types of punctuation marks, whitespaces,etc.\n",
    "    content = content.translate(str.maketrans('','',string.punctuation))\n",
    "    content_words = content.split()\n",
    "    \n",
    "    stop_words = make_stopword_list(stopword_file_list)\n",
    "    #removing all the stop_words present in article content \n",
    "    for word in stop_words:\n",
    "        while word in content_words:\n",
    "            content_words.remove(word)\n",
    "    new_content = ' '.join(content_words)\n",
    "    \n",
    "    # using nltk libraries to use word_tokenize\n",
    "    tokens = word_tokenize(new_content)      \n",
    "    return tokens\n",
    "\n",
    "#functions to generate a positive score of a webpage\n",
    "def positive_score(stopword_file_list, pos_file, content):\n",
    "    positive_dict = make_positive_dict(stopword_file_list, pos_file)\n",
    "    tokens = content_without_stop_words(content)\n",
    "    positive_score = 0\n",
    "    for word in tokens:\n",
    "        if word in positive_dict.values():\n",
    "            positive_score += 1\n",
    "\n",
    "    return positive_score\n",
    "\n",
    "#functions to generate a negative score of a webpage\n",
    "def negative_score(stopword_file_list, neg_file, content):\n",
    "    negative_dict = make_negative_dict(stopword_file_list, neg_file)\n",
    "    tokens = content_without_stop_words(content)\n",
    "    negative_score = 0\n",
    "    for word in tokens:\n",
    "        if word in negative_dict.values():\n",
    "            negative_score -= 1\n",
    "\n",
    "    return negative_score * -1\n",
    "\n",
    "#function to count a numbers of words(including stop words) in a webpage\n",
    "def count_content_words(content):\n",
    "    words = word_tokenize(content)\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    total_no_of_words = len(words)\n",
    "    return total_no_of_words\n",
    "\n",
    "#function to count a numbers of sentences(including stop words) in a webpage\n",
    "def count_content_sentences(content):\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    return len(sentences)\n",
    "\n",
    "#function to count a numbers of syllables(including stop words) in a word\n",
    "#Note: as it specified \"Complex words are words in the text that contain more than two syllables.\"\n",
    "def count_syallbles(word):\n",
    "    if word == \"’\":\n",
    "        return 0\n",
    "    vowel = 'aeiouy'\n",
    "    count = 0\n",
    "    previous_character = False\n",
    "    for character in word:\n",
    "        if character.lower() in vowel:\n",
    "            if not previous_character:\n",
    "                count +=1\n",
    "            previous_character = True\n",
    "        else:\n",
    "            previous_character = False\n",
    "    \n",
    "    if character.endswith(('es','ed')) and not character.endswith(('le','ue')):\n",
    "        count -=1\n",
    "    if character.endswith('e') and not character.endswith(('le','ue')):\n",
    "        count -=1\n",
    "        \n",
    "    # Ensure at least one syllable\n",
    "    count = max(count, 1)\n",
    "    return count\n",
    "\n",
    "#function to count a numbers of complex words(including stop words) in a webpage\n",
    "def count_complex_words(content):\n",
    "    words = word_tokenize(content)\n",
    "    word_list = [word for word in words if word not in string.punctuation]\n",
    "    complex_list = []\n",
    "    for word in word_list:\n",
    "        count = count_syallbles(word)\n",
    "        if count > 2:\n",
    "            complex_list.append(word)\n",
    "    return len(complex_list)\n",
    "\n",
    "#Function to count the personal pronouns in a webpage\n",
    "def count_personal_pronouns(content):\n",
    "    # Define a regular expression pattern to match personal pronouns\n",
    "    pattern = r'\\b(?:I|we|my|our(?:s)?|us)\\b'\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, content, flags=re.IGNORECASE)\n",
    "    \n",
    "    if ('US' in matches):\n",
    "        matches.remove('US')\n",
    "    if ('Us' in matches):\n",
    "        matches.remove('Us')\n",
    "    # Return the count of matches\n",
    "    return len(matches)\n",
    "\n",
    "#Function to calculate the average word length in a webpage\n",
    "def average_word_length(content):\n",
    "    # Remove punctuation from text first\n",
    "    text_without_punctuation = content.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    words = word_tokenize(text_without_punctuation)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    if total_words > 0:\n",
    "        average_length = total_characters / total_words\n",
    "    else:\n",
    "        average_length = 0\n",
    "    \n",
    "    return average_length\n",
    "\n",
    "#Function to count the Syllable Per Word in a webpage\n",
    "def Syllable_Count_Per_Word(content):\n",
    "    text_without_punctuation = content.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text_without_punctuation)\n",
    "    no_of_syallble = {}\n",
    "    for word in words:\n",
    "        if not word.isdigit() and not (word == \"’\"):\n",
    "            no_of_syallble[word] = count_syallbles(word)\n",
    "    syllable_per_word = sum(no_of_syallble.values()) / len(no_of_syallble)\n",
    "    return syllable_per_word\n",
    "\n",
    "#Function to calculate the average of words per sentences\n",
    "def average_words_per_sentence(content):\n",
    "    sentences = sent_tokenize(content)\n",
    "\n",
    "    total_words = 0\n",
    "    for sent in sentences:\n",
    "        words = len(word_tokenize(sent))\n",
    "        total_words += words\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    if total_sentences > 0:\n",
    "        average = total_words / total_sentences\n",
    "    else:\n",
    "        average = 0\n",
    "    \n",
    "    return average\n",
    "\n",
    "#function to save file in Excel\n",
    "def write_to_excel(column_names, row_values, filename='Output_Data_Structure.xlsx'):\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        wb = load_workbook(filename)\n",
    "    except FileNotFoundError:\n",
    "        wb = Workbook()\n",
    "        \n",
    "    ws = wb.active\n",
    "    # If the sheet is empty, write the header row\n",
    "    if ws.max_row == 0:\n",
    "        ws.append(column_names)\n",
    "    ws.append(row_values)\n",
    "    wb.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a885b163-f722-4ad0-9707-dfa2586c6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate all values and store it in Excel file  as \"Output_Data_Structure.xlsx\"\n",
    "\n",
    "def save_output(folder_name, num_of_files):\n",
    "    for i in range(num_of_files):\n",
    "        try:\n",
    "            content_file = folder_name + '/webpage'+ str(\"{:04d}\".format(i+1)) + '.txt'\n",
    "            file_name = 'webpage'+ str(\"{:04d}\".format(i+1))\n",
    "\n",
    "            with open(content_file, 'r') as article:\n",
    "                # there is an anomaly : \"’s\"\n",
    "                content = (article.read()).replace(\"’s\", \"\") \n",
    "\n",
    "            #storing lists of words\n",
    "            stop_words = make_stopword_list(stopword_file_list)\n",
    "            positive_words = make_pos_words_list(pos_file)\n",
    "            negative_words = make_neg_words_list(neg_file)\n",
    "\n",
    "            #storing a dictionary\n",
    "            positive_dict = make_positive_dict(stopword_file_list, pos_file)\n",
    "            negative_dict = make_negative_dict(stopword_file_list, neg_file)\n",
    "\n",
    "            #list of list of words(excluding stop words) in a webpage\n",
    "            tokens = content_without_stop_words(content)\n",
    "\n",
    "            #store a positive and negative store\n",
    "            pos_score = positive_score(stopword_file_list, pos_file, content)\n",
    "            neg_score = negative_score(stopword_file_list, neg_file, content)\n",
    "\n",
    "            #calculate and store Polarity Score and Subjectivity Score\n",
    "            Polarity_Score = (pos_score - neg_score)/ ((pos_score + neg_score) + 0.000001)\n",
    "            Subjectivity_Score = (pos_score + neg_score)/ (len(tokens) + 0.000001)\n",
    "\n",
    "            #store the numbers of complex word, sentences, total words(included complex word) in a webpage\n",
    "            complex_word_count = count_complex_words(content)\n",
    "            content_sentences = count_content_sentences(content)\n",
    "            content_words = count_content_words(content)\n",
    "\n",
    "            #calculate and store Average Sentence Length, Percentage of Complex words and Fog Index\n",
    "            Average_Sentence_Length = content_words / content_sentences\n",
    "            Percentage_of_Complex_words = complex_word_count / content_words\n",
    "            Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "\n",
    "            #calculate and store average words per sentence\n",
    "            avg_words_per_sentence = average_words_per_sentence(content)\n",
    "\n",
    "            #calculate and store personal pronouns in a content\n",
    "            personal_pronouns_count = count_personal_pronouns(content)\n",
    "\n",
    "            #calculate and store average of word length in a content\n",
    "            avg_word_length = average_word_length(content)\n",
    "\n",
    "            #calculate and store Syllable Count Per Word in a content\n",
    "            Syllable_Per_Word = Syllable_Count_Per_Word(content)\n",
    "\n",
    "            #store the output in excel file as \"Output_Data_Structure.xlsx\"\n",
    "            column_names = [\"Input_file_variables\",\"POSITIVE_SCORE\",\"NEGATIVE_SCORE\",\"POLARITY_SCORE\",\"SUBJECTIVITY_SCORE\",\"AVG_SENTENCE_LENGTH\",\"PERCENTAGE_OF_COMPLEX_WORDS\",\"FOG_INDEX\",\"AVG_NUMBER_OF_WORDS_PER_SENTENCE\",\"COMPLEX_WORD_COUNT\",\"WORD_COUNT\",\"SYLLABLE_PER_WORD\",\"PERSONAL_PRONOUNS\",\"AVG_WORD_LENGTH\"]\n",
    "            row_values = [ file_name , pos_score , neg_score , Polarity_Score , Subjectivity_Score , Average_Sentence_Length , Percentage_of_Complex_words , Fog_Index , avg_words_per_sentence , complex_word_count, content_words, Syllable_Per_Word , personal_pronouns_count , avg_word_length ] \n",
    "            write_to_excel(column_names, row_values)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(\"Input file not found:\", file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987fe3a-9692-4d16-8aa7-7de5e994c73c",
   "metadata": {},
   "source": [
    "## Execute Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069e153-a9f6-4dd5-9e81-f57501853f1a",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "##### To avoid duplicacy, first delete the output excel file(if program executed once already), then execute/run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a2cf6b-3bbf-481b-ba1d-68601cb350b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage 0036 have 404 Error\n",
      "Webpage 0049 have 404 Error\n",
      "\n",
      "\n",
      " All 100 webpages loaded as txt in extracted_articles\n",
      "Input file not found: webpage0036\n",
      "Input file not found: webpage0049\n"
     ]
    }
   ],
   "source": [
    "#main function\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    #load the input.xlsx file inside python environment            \n",
    "    #[Note: Each articles store inside \"extracted_articles\" folder with their specified \"URL_ID\"]\n",
    "    Input_file = 'Input.xlsx'\n",
    "    num_of_files = 100\n",
    "    folder_name = 'extracted_articles'\n",
    "    store_articles(Input_file, num_of_files, folder_name) \n",
    "\n",
    "    # Required parameters for functions\n",
    "    stopword_file_list = ['StopWords_Auditor','StopWords_Currencies','StopWords_DatesandNumbers','StopWords_Generic','StopWords_GenericLong','StopWords_Geographic','StopWords_Names']\n",
    "    pos_file = 'MastersDictionary/positive-words.txt'\n",
    "    neg_file = 'MastersDictionary/negative-words.txt'\n",
    "    \n",
    "    #save the file in system as Output_Data_Structure.xlsx (default argument)\n",
    "    save_output(folder_name, num_of_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46262609-87a0-4267-a697-d4977fbf5eec",
   "metadata": {},
   "source": [
    "# Result output\n",
    "* As a code result, it generate an excel file as a \"output_data_structure.xlsx\", then we can download it to our local system.\n",
    "\n",
    "* For Text Analysis we find out these variables here\n",
    "1. All input variables in “Input.xlsx”\n",
    "2. POSITIVE SCORE\n",
    "3. NEGATIVE SCORE\n",
    "4. POLARITY SCORE\n",
    "5. SUBJECTIVITY SCORE\n",
    "6. AVG SENTENCE LENGTH\n",
    "7. PERCENTAGE OF COMPLEX WORDS\n",
    "8. FOG INDEX\n",
    "9. AVG NUMBER OF WORDS PER SENTENCE\n",
    "10. COMPLEX WORD COUNT\n",
    "11. WORD COUNT\n",
    "12. SYLLABLE PER WORD\n",
    "13. PERSONAL PRONOUNS\n",
    "14. AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b6cd6-d9ea-4658-a297-ff85db9bfffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
